INTRODUCTION TO GLSL
====================
glsl is a statically typed imperative programming language

variables must be assigned a type when declared
	3 scalar types:
	bool, int, float
operators
	+,-,*,/,<,>,<=,>=,==,!=
	+=,-=,*=,/=,++,--
procedures
	can decompose shaders into subroutines with C-like syntax
	declaration needs return type or "void" if none

QUALIFIERS AND BUILT-INS
========================

precision specifiers
	floats are declared with an optional precision specifier that tells the GPU how many bits to use
		lowp - lowest possible
		mediump - default precision
		highp - highest possible
	can specify precision of all floats with
		precision highp float; @ top

constants
	const

in/out
	functions return a single value but can simulate multiple returns with reference types
	type qualifiers for procedure arguments
		in - passes argument value (default behavior)
		inout - passes argument by reference
		out - argument uninitialized, but writing to the value updates the parameter
		const - constant value

built-ins
	built in functions some include
		radians, degrees
		sin, cos, tan, asin, acos, atan
		exp, log, exp2, log2
		pow, sqrt, inversesqrt
		floor, ceil, fract, mod, step
		abs, sign, min, max, clamp
		mix - interpolation

VECTORS
=======

built in types for vectors
	bvec2, bvec3, bvec4 - boolean vector
	ivec2, ivec3, ivec4 - integer vector
	vec2, vec3, vec4    - float vector

swizzles

for accessing components of vectors
	First component of p  = p.x = p.r = p.s = p[0]
	Second component of p = p.y = p.g = p.t = p[1]
	Third component of p  = p.z = p.b = p.u = p[2]
	Fourth component of p = p.w = p.a = p.v = p[3]

can select ranges/subtypes with same symbols i.e. p.xxy

arithmetic operations are applied component-wise

geometric functions
	length(p) - euclidian length
	distance(a,b) - euclidian distance
	dot(a,b) - dot product
	cross(a,b) - cross product
	normalize(a) - rescale to unit length
	faceforward(n, I, nr) - reorient a normal to point away from a surface
	reflect(I, N) - reflects vector I along axis N
	refract(I, N, eta) - refracts I according to Snell's law

BRANCHING
=========

use sparingly!!

if statements
	if (a < 0.5) {
	} else {
	}

comparisons
	component-wise comparison that returns a bvec
	lessThan(a,b), lessThanEqual(a,b)
	greaterThan(a,b), greaterThanEqual(a,b)
	equal(a,b)

boolean operations
	aggregate operations
	any(b), all(b), not(b)

LOOPS
=====

loops are supported but the number of loops must be statically determined and bounded
	for(int i=0; i < 100; ++i){
	}
	while(i < 10){
	i++;
	}

MATRICES
========

special datatypes for low dimensional (square matrices)
	mat2, mat3, mat4
	matrix constructors are in column major order

	mat2 I = mat2(1.0, 0.0,
	              0.0, 1.0);
	// same thing
	mat2 I = mat2(1.0);

	can also be constructed by giving columns
	vec2 a = vec2(1,0);
	vec2 b = vec2(0,1);
	mat2 I = mat2(a,b);

	can access columns with square brackets so
	vec2 a = I[0]; // a = (1,0)

arithmetic
	component wise addition  m + w
	scalar multiplication 2 * m
	component-wise multiplication - matrixCompMult(m,w)
	matrix multiplication - m * w




INTRODUCTION TO FRAGMENT SHADERS
================================

a fragment is the color of some fraction of a pixel

entry point is procedure called main(), no arguments or return value
	output is RGBA color values in a builtin array gl_FragData[]
	n-th entry in the gl_FragData[n] array represents the color that will be written to the color attachment at position n. when rendering to the drawing buffer, which has only one color attachment can use gl_FragColor (gl_FragData[0])

	gl_FragCoord - special input to every gragment shader a vec4 which returns the coordinate of the fragment in device coordinates
	gl_FragCoord.xy - coordinate of the fragment in units relative to top-left of the buffer, y - row, x - column
	gl_FragCoord.z - depth value [0,1] 0 - closest, 1 - furthest away
	gl_FragCoord.w - reciprocal of the homogeneous part of the fragment's position in clip coordinates

THE DISCARD KEYWORD
===================

skip rendering of a fragment with the discard statement

UNIFORM VARIABLES AND TEXTURES
==============================

uniform variables are broadcast to all executions of a shader
	useful for small, frequently changing info
textures are 2d arrays of vectors, declared using sampler2D data type and can be accessed with built in function texture2D()
	vec4 texture2D(
	  in sampler2D texture, - sampler variable
	  in vec2 coordinate, - which data is read [0,1]
	  in float bias = 0.0 - changes filtering
	);

INTRODUCTION TO VERTEX SHADERS
==============================

vertex shaders control how geometry is rendered, and are executed before fragment shaders
	a vertex is of of the corners of a primitive
	primitives are simplices of dimension <= 3
		points - 1 vertex
		lines - 2 vertices
		triangles - 3 vertices
	primitives are drawn by linearly interpolating between their vertices and vertex shaders control where the vertices are placed
entry point for vertex shaders is a void procedure called main()
output is written to gl_Position, which controls the placement of the vertex in clip coordinates

attributes
	in addition to getting inputs from uniform variables, vertex shaders can accept per-vertex information via attribute variables

	attribute vec2 position;

	void main() {
	  gl_Position = vec4(position, 0, 1);
	}

VARYING VARIABLES
=================

Connecting vertex shaders to fragment shaders
	vertex shaders can be used to send information directly to fragment shaders using the varying type qualifier
		varying variables are declared at the global scope within a shader
			must be float, vec2, vec3, or vec4
			by default they are linearly interpolated across the rendered primitive

CLIP COORDINATES
================

projective geometry
	gl_Position is a 4d vector because objects on the GPU are represented in a homogeneous coordinate system
	basic idea is to replace points in a space with lines passing through the origin in a space which is one dimension higher since they can be parameterized by vectors

	two vectors identify the same line if for a non-zero scalar multiple t
	   [x0,y0,z0,w0]  ~   [x1,y1,z1,w1]
                 if and only if
		[t*x0,t*y0,t*z0,t*w0]  ==  [x1,y1,z1,w1]

	lines through the origin can be identified with points in a normal Euclidian space by intersecting them with a hyperplane that does not pass through the origin
		in webgl this hyperplane is taken to be the solution set to w=l
		so any vector [x,y,z,w], corresponds to the 3D point [x/w,y/w,z/w]

points at infinity
	there are extra points corresponding to lines that do not pass through the hyperplane, i.e. where w = 0 called "the points at infinity"
	act like idealized direction vectors instead of proper points

clip coordinates on the gpu
	screenColumn = 0.5 * screenWidth  * (gl_Position.x / gl_Position.w + 1.0)
	screenRow    = 0.5 * screenHeight * (1.0 - gl_Position.y / gl_Position.w)
	depth = gl_Position.z / gl_Position.w

	simplifies testing whether or not a point is visible

		-w < x < w
		-w < y < w
		 0 < z < w

transformations
	any change in reference frame in plane geometry can be encoded as a 4x4 matrix in homogenous coordinates
		used to control position and orientation of the camera, the shape of the viewable frustrum and the location of objects within the scene

	vec4 transformPoint(mat4 transform, vec4 point) {
	  return transform * point;
	}

	transformPoint(A, transformPoint(B, p)) == transformPoint(A * B, p)

the model-view-projection factorization
	3d graphical applications make use of 4 different coordinate systems
		data coordinates - vertices in the model
		world coordinates - coordinates of objects in the scene
		view coordinates - unprojected coordinates of the camera
		clip coordinates - coordinates used by the gpu to render all primitives

	relationship between the coordinate systems is specified with 3 diff transformations
		model - transforms the object from data coordinates to world coordinates
			controls the location of the object in the world
		view - transforms world coordinate system to a viewing coordinate system
			controls the position and orientation of the camera
		projection - transforms the view coordinate system into device clip coordinates
			controls whether the view is orthographic or perspective and the aspect ratio of the camera

TRANSLATIONS
============

a translation of a vector v moving the point o to the origin is a function
	vec3 translatePoint(vec3 v, vec3 o) {
	  return v - o;
	}
but translations are not linear in affine coordinates, but they are in projective homogeneous coordinates
	so they can be written as a matrix

SCALING
=======

scaling transformations stretch or squish the coordinate axes
scaling a vector v by a factor of s along each axis can be define as follows
	vec3 scaleVector(vec3 v, vec3 s) {
	  return s * v;
	}
also linear in projective geometry

REFLECTIONS
===========

reflections flip a vector along a given axis
reflection of a point p along the axis n as follows
	vec3 reflectPoint(vec3 p, vec3 n) {
	  return p - 2.0 * dot(n, p) * n / dot(n, n);
	}

ROTATIONS
=========

a rotation is the composition of an even number of reflections
	axis-angle notiation - any 3d rotation can be represented as a rotation in a plane about some axis since all 3d rotations can be written as product of exactly 2 reflections
	the common line is called the axis of the rotation, angle between the two planes is called the angle of rotation

rotation of a point p about the unit axis n with angle theta as follows
	vec3 rotatePoint(vec3 p, vec3 n, float theta) {
	  return (
	    p * cos(theta) + cross(n, p) *
	    sin(theta) + n * dot(p, n) *
	    (1.0 - cos(theta))
	  );
	}

LIGHTING MODELS
===============

physically, images are formed by the interaction of lightwaves with some detector. these lightwaves are formed by the interaction of the electromagnetic field with different materials.
interactions include
	reflection, transmission, absorption, and emission of electromagnetic radiation
visible light waves are high frequency and travel at fast speeds. as a result, physical images can be well approximated using geometrical objects, which replaces light waves with rays.
a ray in geometric optics is a line perpendicular to the wave front representing the amount of energy in the wave at some particular frequency, ignoring polarization.
natural light is composed of infinitely many different frequencies, however human vision can only distingush between three different classes of frequencies: r/g/b, so when rendering images we only need to track the amount of energy in the rgb bands
in high end ocmputer graphics, ray tracing is widely used to generate physically realistic images. real time applications usually make do with more limited models of light transport and interactions.

DIFFUSE LIGHTING
================

Lambertian diffuse lighting
	a simple physically based model for matte surfaces.
	assume that the surface will absorb some portion of the incoming radiation, and scatter the rest uniformly.

assuming that the light source is far enough away for all incoming light to be travelling in the same direction d. Lambert's cosine law says that the amount of diffuse light emitted from a point on the surface with a normal vector n is proportional to the following weight:
	float lambertWeight(vec3 n, vec3 d) {
	  return max(dot(n, d), 0.0);
	}
combined with an ambient term
	vec3 reflectedLight(
	  vec3 normal,
	  vec3 lightDirection,
	  vec3 ambient,
	  vec3 diffuse
	) {
	  float brightness = dot(normal, lightDirection);
	  return ambient + diffuse * max(brightness, 0.0);
	}
normal vectors are dual vectors, they encode planes parallel to the surface, not directions. the parallel distance along a normal vector is given by the dot product with the normal vector
given a test point p, the distance along the normal direction at a point on the surface is given by the following function:
	float parallelDistance(
	  vec3 surfacePoint,
	  vec3 surfaceNormal,
	  vec3 p
	) {
	  return dot(p - surfacePoint, surfaceNormal);
	}
if we transform the surface into world coordinates using a model transformation then the input test point is moved by the inverse of model.
in order for the above function to remain invariant, the surface normal must transform by the inverse transpose of model

PHONG LIGHTING
==============

the Phong Lighting model is probably the most widely used lighting model in computer graphics. it models shiny or specular materials like metals & plastics. physically, these materials scatter light by hard reflections instead of smoothly diffusing outward, the Phong Lighting model approximates this process by adding in an extra term to account for these reflections.

first reflect the incoming light direction about the surface normal, then project this vector onto the view axis and measure its length. the amount of reflected specular light is then assumed to be porportional to some power of this length

	float phongWeight(
	  vec3 lightDirection,
	  vec3 surfaceNormal,
	  vec3 eyeDirection,
	  float shininess
	) {
	  //First reflect light by surface normal
	  vec3 rlight = reflect(lightDirection, surfaceNormal);

	  //Next find the projected length of the reflected
	  //light vector in the view direction
	  float eyeLight = max(dot(rlight, eyeDirection), 0.0);

	  //Finally exponentiate by the shininess factor
	  return pow(eyeLight, shininess);
	}

	light = (
	    ambient
	  + diffuse * lambert
	  + specular * phong
	)

POINT LIGHTS
============

up til now we've assumed that all our light sources are infinitely far away and that their geometry can be modelled by a single direction vector
instead generalize to the situation where the lights are represented by idealized points which emit light uniformly in all directions

replace the direction vector with a ray extending from the point on the surface to the light source:
	vec3 lightDirection = normalize(
	  lightPosition - surfacePosition
	);

MULTIPLE LIGHT SOURCES
======================

the phong lighting model can be extended to suppor tmultiple lights by summing up their individual contributions
if light0 and light2 are light values from two different sources, their comined light value is:
	vec3 combinedLight(vec3 light0, vec3 light1) {
	  return light0 + light1;
	}

for the phong lighting model, the ambient component is factored out and each individual point light is given a separate diffuse and specular component.

structs
	to simplify describing multiple lights, can use a struct
	declared using the struct keyword
	for example a struct for a point light source:
		//Declare a datatype for a point light
		struct PointLight {
		  vec3 diffuse;
		  vec3 specular;
		  vec3 position;
		  float shininess;
		};

		//Declare a single point light called light
		PointLight light;

		//Set the color of the light to red (1,0,0)
		light.color = vec3(1, 0, 0);

arrays
	glsl also supports arrays using the same syntax as C, 0-indexed, but size must be declared in advance
	ex declare 10 point lights:
		//Declare an array of 10 point lights
		//called "lights"
		PointLight lights[10];

		//Modify the first light in the array
		lights[0].radius = 100.0;

NON-PHOTOREALISTIC RENDERING
============================

Cel shading

physically based models of lighting are good when the goal is to create realistic images, but cel-shading is used to flatten the colors of an image giving it a more cartoony hand drawn look.

the basic idea behind cel-shading is to start from the lambert diffuse lighting model, and then apply quantization to intensity values.
for example if we want to round our light value into one of 8 different buckets which are uniformly sized:
	float celIntensity = ceil(lightIntensity * 8.0) / 8.0;
then we would use celIntensity instead of lightIntensity in the rest of the calculations within our lighting model.

GOOCH SHADING
=============

another simple non-photorealistic rendering techinque is gooch shading. useful for technical illustrations or otherwise coloring objects in such a way to make fine details and contours pop out.
the basic idea is to modify lambertian diffuse lighting in two ways
	instead of clamping negative weights to 0, weights can be [-1,1]
	instead of interpolating from the diffuse light value to 0, the light color is smoothly interpolated over some other color space

the weight for the light color in gooch shading is defined to be:
	float goochWeight(vec3 normal, vec3 lightDirection) {
	  return 0.5 * (1.0 + dot(normal, lightDirection));
	}

in two color Gooch shading, the color is given by interpolating between any pair of colors given this weight:
	vec3 goochColor(vec3 cool, vec3 warm, float weight) {
	  return (1.0 - weight) * cool + weight * warm;
	}
it is also possible to interpolate over more colors or to use textures for assigning the colors instead.
